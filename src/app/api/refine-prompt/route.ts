import { GoogleGenAI, HarmCategory, HarmBlockThreshold } from "@google/genai";
import OpenAI from "openai";
import { NextRequest, NextResponse } from "next/server";
import { getProviderForModel, ModelProvider } from "@/lib/custom-models";

const REFINE_SYSTEM_PROMPT = `You are the world's leading expert in Prompt Engineering. Your sole mission is to generate the perfect, ready-to-use prompt from the projects described by the user. You are not a teacher, but a highly specialized service provider who delivers a flawless end product.

**Your Approach:**

1.  **Project Analysis:** You receive the user's description and analyze it for completeness and clarity.

2.  **Targeted Follow-up Questions (if necessary):** Your top priority is to collect all necessary details before creating the prompt. If the user's description is vague or incomplete, you ask precise follow-up questions to clarify all relevant variables. Internally use frameworks like CO-STAR (Context, Objective, Style, Tone, Audience, Response) without explicitly mentioning them. You ask for:
    *   The exact goal (What should the end result be?)
    *   The context (Background information)
    *   The target audience (Who is the output for?)
    *   The desired format and structure (Table, list, code, prose, etc.)
    *   The tone and style (e.g., formal, humorous, scientific)
    *   Constraints or taboos (What must absolutely not be included?)

3.  **Generation of the Master Prompt:** Once you have all the required information, you create a masterfully formulated, structured, and comprehensive prompt. This is designed to be optimally interpreted by a Large Language Model to produce the user's desired result with maximum precision and quality.

**Your Output:**

*   Your only result is the final, finished prompt.
*   You always present this prompt in a markdown code block to facilitate copying.
*   You provide absolutely no explanations, comments, or analyses regarding your approach or the created prompt. Your work is the product itself.`;

interface RefineRequest {
  prompt: string;
  model: string;
}

const safetySettings = [
  {
    category: HarmCategory.HARM_CATEGORY_HARASSMENT,
    threshold: HarmBlockThreshold.BLOCK_NONE,
  },
  {
    category: HarmCategory.HARM_CATEGORY_HATE_SPEECH,
    threshold: HarmBlockThreshold.BLOCK_NONE,
  },
  {
    category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
    threshold: HarmBlockThreshold.BLOCK_NONE,
  },
  {
    category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
    threshold: HarmBlockThreshold.BLOCK_NONE,
  },
  {
    category: HarmCategory.HARM_CATEGORY_CIVIC_INTEGRITY,
    threshold: HarmBlockThreshold.BLOCK_NONE,
  },
];

async function handleGeminiRefine(model: string, userPrompt: string): Promise<Response> {
  const cloudProjectId = process.env.GOOGLE_CLOUD_PROJECT;
  const location = process.env.GOOGLE_CLOUD_LOCATION || "global";

  if (!cloudProjectId) {
    return NextResponse.json({ error: "GOOGLE_CLOUD_PROJECT is not configured." }, { status: 500 });
  }

  const genAI = new GoogleGenAI({ vertexai: true, project: cloudProjectId, location: location });

  const streamingResult = await genAI.models.generateContentStream({
    model,
    contents: [{ role: "user", parts: [{ text: userPrompt }] }],
    config: {
      systemInstruction: REFINE_SYSTEM_PROMPT,
      safetySettings,
    },
  });

  const encoder = new TextEncoder();
  const readableStream = new ReadableStream({
    async start(controller) {
      try {
        for await (const chunk of streamingResult) {
          if (chunk.candidates && chunk.candidates.length > 0) {
            const candidate = chunk.candidates[0];
            if (candidate.content?.parts) {
              for (const part of candidate.content.parts) {
                if (part.text && !part.thought) {
                  const jsonChunk = { type: "text", value: part.text };
                  controller.enqueue(encoder.encode(JSON.stringify(jsonChunk) + "\n"));
                }
              }
            }
          }
        }
      } catch (streamError) {
        console.error("Error during Gemini stream processing:", streamError);
        const errorMessage = {
          type: "error",
          value: "An error occurred during refinement. Please try again.",
        };
        controller.enqueue(encoder.encode(JSON.stringify(errorMessage) + "\n"));
      } finally {
        controller.close();
      }
    },
    cancel() {
      console.log("Gemini refinement stream cancelled");
    },
  });

  return new Response(readableStream, {
    headers: {
      "Content-Type": "application/jsonl; charset=utf-8",
      "Cache-Control": "no-cache",
      Connection: "keep-alive",
    },
  });
}

async function handleOpenAIRefine(model: string, userPrompt: string): Promise<Response> {
  const apiKey = process.env.OPENAI_API_KEY;

  if (!apiKey) {
    return NextResponse.json({ error: "OPENAI_API_KEY is not configured." }, { status: 500 });
  }

  const openai = new OpenAI({ apiKey });

  const stream = await openai.chat.completions.create({
    model,
    messages: [
      { role: "system", content: REFINE_SYSTEM_PROMPT },
      { role: "user", content: userPrompt },
    ],
    stream: true,
  });

  const encoder = new TextEncoder();
  const readableStream = new ReadableStream({
    async start(controller) {
      try {
        for await (const chunk of stream) {
          const delta = chunk.choices[0]?.delta;
          if (delta?.content) {
            const jsonChunk = { type: "text", value: delta.content };
            controller.enqueue(encoder.encode(JSON.stringify(jsonChunk) + "\n"));
          }
        }
      } catch (streamError) {
        console.error("Error during OpenAI stream processing:", streamError);
        const errorMessage = {
          type: "error",
          value: "An error occurred during refinement. Please try again.",
        };
        controller.enqueue(encoder.encode(JSON.stringify(errorMessage) + "\n"));
      } finally {
        controller.close();
      }
    },
    cancel() {
      console.log("OpenAI refinement stream cancelled");
    },
  });

  return new Response(readableStream, {
    headers: {
      "Content-Type": "application/jsonl; charset=utf-8",
      "Cache-Control": "no-cache",
      Connection: "keep-alive",
    },
  });
}

export async function POST(request: NextRequest) {
  const userIdHeader = request.headers.get("x-user-id");
  if (!userIdHeader) {
    return NextResponse.json({ error: "Unauthorized: Missing user identification" }, { status: 401 });
  }
  const userId = parseInt(userIdHeader, 10);
  if (isNaN(userId)) {
    return NextResponse.json({ error: "Invalid user ID" }, { status: 401 });
  }

  const { prompt, model } = (await request.json()) as RefineRequest;

  if (!prompt || !prompt.trim()) {
    return NextResponse.json({ error: "Prompt is required" }, { status: 400 });
  }

  if (!model) {
    return NextResponse.json({ error: "Model is required" }, { status: 400 });
  }

  const provider: ModelProvider = getProviderForModel(model) ?? "gemini";

  try {
    if (provider === "openai") {
      return await handleOpenAIRefine(model, prompt);
    } else {
      return await handleGeminiRefine(model, prompt);
    }
  } catch (error: unknown) {
    console.error(`Error in ${provider} refinement API call:`, error);

    let detailedError = "An unknown error occurred during refinement.";
    let status = 500;

    if (typeof error === "object" && error !== null) {
      if ("status" in error && typeof (error as { status: unknown }).status === "number") {
        status = (error as { status: number }).status;
      }

      if ("message" in error && typeof (error as { message: unknown }).message === "string") {
        let errorMessage = (error as { message: string }).message;
        try {
          const match = errorMessage.match(/{.*}/s);
          if (match && match[0]) {
            const jsonError = JSON.parse(match[0]);
            if (jsonError.error && jsonError.error.message) {
              errorMessage = jsonError.error.message;
            }
          }
        } catch {
          console.warn("Could not parse nested JSON from error message.");
        }
        detailedError = errorMessage;
      }
    } else {
      detailedError = String(error);
    }

    return NextResponse.json({ error: detailedError }, { status });
  }
}
